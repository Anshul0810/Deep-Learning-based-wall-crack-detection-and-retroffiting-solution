{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 349
    },
    "id": "kR8ieO7GAQAh",
    "outputId": "d1f1134c-65dd-41e5-e171-9fce35953a7c"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "u4mfOZ-uAWkl"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.17.0'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "kNQPFbNBAXG2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 9704 images belonging to 5 classes.\n",
      "Found 991 images belonging to 5 classes.\n"
     ]
    }
   ],
   "source": [
    "train_datagen = ImageDataGenerator(rescale = 1./255,\n",
    "                                   shear_range = 0.2,\n",
    "                                   zoom_range = 0.2,\n",
    "                                   horizontal_flip = True)\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1./255,                 # Normalize pixel values to [0, 1]\n",
    "    rotation_range=10,              # Small rotations to simulate natural variations\n",
    "    shear_range=0.2,            # Apply shear transformations (within limits)\n",
    "    width_shift_range=0.1,          # Horizontal translations to augment positional variations\n",
    "    height_shift_range=0.1,         # Vertical translations to augment positional variations\n",
    "    zoom_range=0.1,                 # Small zoom to simulate varying distances\n",
    "    brightness_range=(0.8, 1.2),    # Simulate different lighting conditions\n",
    "    fill_mode='nearest'             # Fill pixels outside boundaries with nearest pixel value\n",
    ")\n",
    "training_set = train_datagen.flow_from_directory('Dataset-Wall-crack-detection',\n",
    "                                                 target_size = (64, 64),\n",
    "                                                 batch_size = 32,\n",
    "                                                 class_mode = 'binary')\n",
    "test_datagen = ImageDataGenerator(rescale = 1./255)\n",
    "test_set = train_datagen.flow_from_directory('test_set',\n",
    "                                                 target_size = (64, 64),\n",
    "                                                 batch_size = 32,\n",
    "                                                 class_mode = 'binary')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 159
    },
    "id": "XPzPrMckl-hV",
    "outputId": "fc2ffcb1-f8f4-4fe2-b160-cbfea9ccfecf"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lenovo\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\convolutional\\base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "# Initializing CNN\n",
    "cnn = tf.keras.models.Sequential()\n",
    "\n",
    "# Define the input layer explicitly\n",
    "cnn.add(tf.keras.layers.Input(shape=(128, 128, 3)))  # Input layer\n",
    "\n",
    "# Step 1 - Convolution + Pooling\n",
    "cnn.add(tf.keras.layers.Conv2D(filters=64, kernel_size=3, activation='relu', input_shape=[128, 128, 3]))\n",
    "cnn.add(tf.keras.layers.MaxPool2D(pool_size=2, strides=2))\n",
    "\n",
    "# Step 2 - Additional Convolution + Pooling Layers\n",
    "cnn.add(tf.keras.layers.Conv2D(filters=128, kernel_size=3, activation='relu'))\n",
    "cnn.add(tf.keras.layers.MaxPool2D(pool_size=2, strides=2))\n",
    "\n",
    "# Step 3 - Additional Convolution + Pooling Layers\n",
    "cnn.add(tf.keras.layers.Conv2D(filters=256, kernel_size=3, activation='relu'))\n",
    "cnn.add(tf.keras.layers.MaxPool2D(pool_size=2, strides=2))\n",
    "\n",
    "# Apply GlobalAveragePooling2D\n",
    "cnn.add(tf.keras.layers.GlobalAveragePooling2D())\n",
    "\n",
    "# Fully Connected Layers\n",
    "cnn.add(tf.keras.layers.Dense(units=256, activation='relu'))  # Fully connected layer\n",
    "cnn.add(tf.keras.layers.Dropout(0.5))  # Dropout for regularization\n",
    "cnn.add(tf.keras.layers.Dense(units=5, activation='softmax'))  # 5 classes for classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "d8wBXCf9KXPF"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lenovo\\anaconda3\\Lib\\site-packages\\keras\\src\\trainers\\data_adapters\\py_dataset_adapter.py:122: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m304/304\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m167s\u001b[0m 529ms/step - accuracy: 0.3296 - loss: 1.4314 - val_accuracy: 0.7175 - val_loss: 0.7109\n",
      "Epoch 2/25\n",
      "\u001b[1m304/304\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m63s\u001b[0m 205ms/step - accuracy: 0.6659 - loss: 0.7982 - val_accuracy: 0.7134 - val_loss: 0.7218\n",
      "Epoch 3/25\n",
      "\u001b[1m304/304\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 193ms/step - accuracy: 0.7007 - loss: 0.7387 - val_accuracy: 0.7608 - val_loss: 0.6236\n",
      "Epoch 4/25\n",
      "\u001b[1m304/304\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 198ms/step - accuracy: 0.7155 - loss: 0.6864 - val_accuracy: 0.7225 - val_loss: 0.6782\n",
      "Epoch 5/25\n",
      "\u001b[1m304/304\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 194ms/step - accuracy: 0.7140 - loss: 0.6878 - val_accuracy: 0.7467 - val_loss: 0.6135\n",
      "Epoch 6/25\n",
      "\u001b[1m304/304\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 198ms/step - accuracy: 0.7301 - loss: 0.6590 - val_accuracy: 0.7750 - val_loss: 0.5673\n",
      "Epoch 7/25\n",
      "\u001b[1m304/304\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m75s\u001b[0m 243ms/step - accuracy: 0.7405 - loss: 0.6345 - val_accuracy: 0.7679 - val_loss: 0.5591\n",
      "Epoch 8/25\n",
      "\u001b[1m304/304\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m147s\u001b[0m 473ms/step - accuracy: 0.7509 - loss: 0.6171 - val_accuracy: 0.7659 - val_loss: 0.5695\n",
      "Epoch 9/25\n",
      "\u001b[1m304/304\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 198ms/step - accuracy: 0.7573 - loss: 0.6044 - val_accuracy: 0.7851 - val_loss: 0.5490\n",
      "Epoch 10/25\n",
      "\u001b[1m304/304\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 198ms/step - accuracy: 0.7571 - loss: 0.6035 - val_accuracy: 0.7810 - val_loss: 0.5468\n",
      "Epoch 11/25\n",
      "\u001b[1m304/304\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m62s\u001b[0m 201ms/step - accuracy: 0.7530 - loss: 0.6020 - val_accuracy: 0.8052 - val_loss: 0.5322\n",
      "Epoch 12/25\n",
      "\u001b[1m304/304\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 196ms/step - accuracy: 0.7507 - loss: 0.6082 - val_accuracy: 0.8093 - val_loss: 0.4906\n",
      "Epoch 13/25\n",
      "\u001b[1m304/304\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 200ms/step - accuracy: 0.7701 - loss: 0.5726 - val_accuracy: 0.7830 - val_loss: 0.5420\n",
      "Epoch 14/25\n",
      "\u001b[1m304/304\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 195ms/step - accuracy: 0.7652 - loss: 0.5771 - val_accuracy: 0.8032 - val_loss: 0.5125\n",
      "Epoch 15/25\n",
      "\u001b[1m304/304\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m62s\u001b[0m 202ms/step - accuracy: 0.7697 - loss: 0.5660 - val_accuracy: 0.8113 - val_loss: 0.5149\n",
      "Epoch 16/25\n",
      "\u001b[1m304/304\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m62s\u001b[0m 202ms/step - accuracy: 0.7724 - loss: 0.5662 - val_accuracy: 0.7730 - val_loss: 0.5615\n",
      "Epoch 17/25\n",
      "\u001b[1m304/304\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m62s\u001b[0m 201ms/step - accuracy: 0.7807 - loss: 0.5577 - val_accuracy: 0.8224 - val_loss: 0.4735\n",
      "Epoch 18/25\n",
      "\u001b[1m304/304\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 198ms/step - accuracy: 0.7764 - loss: 0.5648 - val_accuracy: 0.8022 - val_loss: 0.5267\n",
      "Epoch 19/25\n",
      "\u001b[1m304/304\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m62s\u001b[0m 200ms/step - accuracy: 0.7783 - loss: 0.5486 - val_accuracy: 0.7982 - val_loss: 0.5071\n",
      "Epoch 20/25\n",
      "\u001b[1m304/304\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 197ms/step - accuracy: 0.7808 - loss: 0.5550 - val_accuracy: 0.8022 - val_loss: 0.4911\n",
      "Epoch 21/25\n",
      "\u001b[1m304/304\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m62s\u001b[0m 202ms/step - accuracy: 0.7891 - loss: 0.5339 - val_accuracy: 0.7881 - val_loss: 0.5239\n",
      "Epoch 22/25\n",
      "\u001b[1m304/304\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 198ms/step - accuracy: 0.7843 - loss: 0.5435 - val_accuracy: 0.8204 - val_loss: 0.4688\n",
      "Epoch 23/25\n",
      "\u001b[1m304/304\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m62s\u001b[0m 202ms/step - accuracy: 0.7939 - loss: 0.5284 - val_accuracy: 0.8022 - val_loss: 0.5126\n",
      "Epoch 24/25\n",
      "\u001b[1m304/304\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 198ms/step - accuracy: 0.7923 - loss: 0.5286 - val_accuracy: 0.7901 - val_loss: 0.5181\n",
      "Epoch 25/25\n",
      "\u001b[1m304/304\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m63s\u001b[0m 205ms/step - accuracy: 0.7904 - loss: 0.5265 - val_accuracy: 0.8254 - val_loss: 0.4836\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x22f5fa85fa0>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compiling the CNN\n",
    "cnn.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Training the CNN on the Training set and evaluating it on the Test set\n",
    "cnn.fit(x=training_set, validation_data=test_set, epochs=25)  # Increased epochs for better learning\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "d8wBXCf9KXPF"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n",
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "# Save CNN Model\n",
    "cnn.save(\"cnn_crack_detection.h5\")\n",
    "\n",
    "# Load CNN Model\n",
    "cnn = load_model(\"cnn_crack_detection.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VGG16 Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.applications import VGG16\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten, Dropout, GlobalAveragePooling2D\n",
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing import image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 9704 images belonging to 5 classes.\n",
      "Found 991 images belonging to 5 classes.\n"
     ]
    }
   ],
   "source": [
    "# DATA PREPROCESSING\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    rotation_range=10,\n",
    "    shear_range=0.2,\n",
    "    width_shift_range=0.1,\n",
    "    height_shift_range=0.1,\n",
    "    zoom_range=0.1,\n",
    "    brightness_range=(0.8, 1.2),\n",
    "    fill_mode='nearest'\n",
    ")\n",
    "\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "training_set = train_datagen.flow_from_directory(\n",
    "    'Dataset-Wall-crack-detection',\n",
    "    target_size=(128, 128),  # VGG16 input size\n",
    "    batch_size=64,\n",
    "    class_mode='categorical'\n",
    ")\n",
    "\n",
    "test_set = test_datagen.flow_from_directory(\n",
    "    'test_set',\n",
    "    target_size=(128, 128),\n",
    "    batch_size=64,\n",
    "    class_mode='categorical'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOAD PRETRAINED VGG16 MODEL (Without Fully Connected Layers)\n",
    "base_model = VGG16(weights='imagenet', include_top=False, input_shape=(128, 128, 3))\n",
    "\n",
    "# Freeze the convolutional layers\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = False  # Prevent updating weights\n",
    "\n",
    "# BUILD CUSTOM MODEL\n",
    "model_vgg16 = Sequential([\n",
    "    base_model,\n",
    "    GlobalAveragePooling2D(),  # Reduces dimensions\n",
    "    Dense(128, activation='relu'),\n",
    "    Dropout(0.5),  # Reduce overfitting\n",
    "    Dense(5, activation='softmax')  # 8 crack types\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m446s\u001b[0m 3s/step - accuracy: 0.4902 - loss: 1.2642 - val_accuracy: 0.7810 - val_loss: 0.6428\n",
      "Epoch 2/25\n",
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m442s\u001b[0m 3s/step - accuracy: 0.7197 - loss: 0.7080 - val_accuracy: 0.8113 - val_loss: 0.5438\n",
      "Epoch 3/25\n",
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m445s\u001b[0m 3s/step - accuracy: 0.7434 - loss: 0.6454 - val_accuracy: 0.8224 - val_loss: 0.5034\n",
      "Epoch 4/25\n",
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m446s\u001b[0m 3s/step - accuracy: 0.7670 - loss: 0.5985 - val_accuracy: 0.8264 - val_loss: 0.4719\n",
      "Epoch 5/25\n",
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m448s\u001b[0m 3s/step - accuracy: 0.7805 - loss: 0.5663 - val_accuracy: 0.8385 - val_loss: 0.4653\n",
      "Epoch 6/25\n",
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m448s\u001b[0m 3s/step - accuracy: 0.7800 - loss: 0.5687 - val_accuracy: 0.8295 - val_loss: 0.4616\n",
      "Epoch 7/25\n",
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m444s\u001b[0m 3s/step - accuracy: 0.7875 - loss: 0.5484 - val_accuracy: 0.8406 - val_loss: 0.4422\n",
      "Epoch 8/25\n",
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m445s\u001b[0m 3s/step - accuracy: 0.7884 - loss: 0.5391 - val_accuracy: 0.8355 - val_loss: 0.4442\n",
      "Epoch 9/25\n",
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m447s\u001b[0m 3s/step - accuracy: 0.7973 - loss: 0.5251 - val_accuracy: 0.8426 - val_loss: 0.4391\n",
      "Epoch 10/25\n",
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m445s\u001b[0m 3s/step - accuracy: 0.7895 - loss: 0.5327 - val_accuracy: 0.8365 - val_loss: 0.4419\n",
      "Epoch 11/25\n",
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m446s\u001b[0m 3s/step - accuracy: 0.8064 - loss: 0.5070 - val_accuracy: 0.8476 - val_loss: 0.4281\n",
      "Epoch 12/25\n",
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m443s\u001b[0m 3s/step - accuracy: 0.8026 - loss: 0.5149 - val_accuracy: 0.8557 - val_loss: 0.4202\n",
      "Epoch 13/25\n",
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m454s\u001b[0m 3s/step - accuracy: 0.8114 - loss: 0.4948 - val_accuracy: 0.8385 - val_loss: 0.4453\n",
      "Epoch 14/25\n",
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m452s\u001b[0m 3s/step - accuracy: 0.8094 - loss: 0.5028 - val_accuracy: 0.8527 - val_loss: 0.4243\n",
      "Epoch 15/25\n",
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m447s\u001b[0m 3s/step - accuracy: 0.8034 - loss: 0.5107 - val_accuracy: 0.8446 - val_loss: 0.4328\n",
      "Epoch 16/25\n",
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m446s\u001b[0m 3s/step - accuracy: 0.8041 - loss: 0.5045 - val_accuracy: 0.8406 - val_loss: 0.4415\n",
      "Epoch 17/25\n",
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m446s\u001b[0m 3s/step - accuracy: 0.8112 - loss: 0.4961 - val_accuracy: 0.8486 - val_loss: 0.4056\n",
      "Epoch 18/25\n",
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m443s\u001b[0m 3s/step - accuracy: 0.8164 - loss: 0.4838 - val_accuracy: 0.8486 - val_loss: 0.4177\n",
      "Epoch 19/25\n",
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m446s\u001b[0m 3s/step - accuracy: 0.8119 - loss: 0.5025 - val_accuracy: 0.8517 - val_loss: 0.4132\n",
      "Epoch 20/25\n",
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m445s\u001b[0m 3s/step - accuracy: 0.8101 - loss: 0.4900 - val_accuracy: 0.8234 - val_loss: 0.4585\n",
      "Epoch 21/25\n",
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m446s\u001b[0m 3s/step - accuracy: 0.8152 - loss: 0.4906 - val_accuracy: 0.8385 - val_loss: 0.4163\n",
      "Epoch 22/25\n",
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m447s\u001b[0m 3s/step - accuracy: 0.8093 - loss: 0.4791 - val_accuracy: 0.8507 - val_loss: 0.4096\n",
      "Epoch 23/25\n",
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m444s\u001b[0m 3s/step - accuracy: 0.8140 - loss: 0.4823 - val_accuracy: 0.8507 - val_loss: 0.4172\n",
      "Epoch 24/25\n",
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m445s\u001b[0m 3s/step - accuracy: 0.8125 - loss: 0.4825 - val_accuracy: 0.8365 - val_loss: 0.4345\n",
      "Epoch 25/25\n",
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m446s\u001b[0m 3s/step - accuracy: 0.8111 - loss: 0.4853 - val_accuracy: 0.8507 - val_loss: 0.4184\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x22f61423380>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compile the model\n",
    "model_vgg16.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# TRAIN THE MODEL\n",
    "model_vgg16.fit(training_set, validation_data=test_set, epochs=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n",
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    }
   ],
   "source": [
    "# Save VGG16 Model\n",
    "model_vgg16.save(\"vgg16_crack_detection.h5\")  # This \"model\" is your VGG16 model\n",
    "\n",
    "# Load VGG16 Model\n",
    "model_vgg16 = load_model(\"vgg16_crack_detection.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ResNet50 Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.applications import ResNet50\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten, Dropout, GlobalAveragePooling2D\n",
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing import image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 9704 images belonging to 5 classes.\n",
      "Found 991 images belonging to 5 classes.\n"
     ]
    }
   ],
   "source": [
    "# DATA PREPROCESSING\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    rotation_range=10,\n",
    "    shear_range=0.2,\n",
    "    width_shift_range=0.1,\n",
    "    height_shift_range=0.1,\n",
    "    zoom_range=0.1,\n",
    "    brightness_range=(0.8, 1.2),\n",
    "    fill_mode='nearest'\n",
    ")\n",
    "\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "training_set = train_datagen.flow_from_directory(\n",
    "    'Dataset-Wall-crack-detection',\n",
    "    target_size=(128, 128),  # ResNet50 input size\n",
    "    batch_size=64,\n",
    "    class_mode='categorical'\n",
    ")\n",
    "\n",
    "test_set = test_datagen.flow_from_directory(\n",
    "    'test_set',\n",
    "    target_size=(128, 128),\n",
    "    batch_size=64,\n",
    "    class_mode='categorical'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOAD PRETRAINED ResNet50 MODEL (Without Fully Connected Layers)\n",
    "base_model_resnet50 = ResNet50(weights='imagenet', include_top=False, input_shape=(128, 128, 3))\n",
    "\n",
    "# Freeze the convolutional layers\n",
    "for layer in base_model_resnet50.layers:\n",
    "    layer.trainable = False  # Prevent updating weights\n",
    "\n",
    "# BUILD CUSTOM MODEL\n",
    "model_resnet50 = Sequential([\n",
    "    base_model_resnet50,\n",
    "    GlobalAveragePooling2D(),  # Reduces dimensions efficiently\n",
    "    Dense(128, activation='relu'),\n",
    "    Dropout(0.5),  # Reduce overfitting\n",
    "    Dense(5, activation='softmax')  # 5 crack types\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m290s\u001b[0m 2s/step - accuracy: 0.2677 - loss: 1.6175 - val_accuracy: 0.3935 - val_loss: 1.4581\n",
      "Epoch 2/25\n",
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m279s\u001b[0m 2s/step - accuracy: 0.4529 - loss: 1.4142 - val_accuracy: 0.6418 - val_loss: 1.2099\n",
      "Epoch 3/25\n",
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m277s\u001b[0m 2s/step - accuracy: 0.5348 - loss: 1.2313 - val_accuracy: 0.6529 - val_loss: 1.0600\n",
      "Epoch 4/25\n",
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m276s\u001b[0m 2s/step - accuracy: 0.5579 - loss: 1.1225 - val_accuracy: 0.6781 - val_loss: 0.9513\n",
      "Epoch 5/25\n",
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m278s\u001b[0m 2s/step - accuracy: 0.6016 - loss: 1.0475 - val_accuracy: 0.6953 - val_loss: 0.9014\n",
      "Epoch 6/25\n",
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m368s\u001b[0m 2s/step - accuracy: 0.6145 - loss: 0.9897 - val_accuracy: 0.7003 - val_loss: 0.8571\n",
      "Epoch 7/25\n",
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m287s\u001b[0m 2s/step - accuracy: 0.6274 - loss: 0.9626 - val_accuracy: 0.6761 - val_loss: 0.8396\n",
      "Epoch 8/25\n",
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m276s\u001b[0m 2s/step - accuracy: 0.6455 - loss: 0.9125 - val_accuracy: 0.6993 - val_loss: 0.8320\n",
      "Epoch 9/25\n",
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m278s\u001b[0m 2s/step - accuracy: 0.6398 - loss: 0.9076 - val_accuracy: 0.6942 - val_loss: 0.8014\n",
      "Epoch 10/25\n",
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m278s\u001b[0m 2s/step - accuracy: 0.6420 - loss: 0.9049 - val_accuracy: 0.7013 - val_loss: 0.7722\n",
      "Epoch 11/25\n",
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m276s\u001b[0m 2s/step - accuracy: 0.6401 - loss: 0.8953 - val_accuracy: 0.7064 - val_loss: 0.7590\n",
      "Epoch 12/25\n",
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m277s\u001b[0m 2s/step - accuracy: 0.6621 - loss: 0.8514 - val_accuracy: 0.7114 - val_loss: 0.7461\n",
      "Epoch 13/25\n",
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m278s\u001b[0m 2s/step - accuracy: 0.6586 - loss: 0.8622 - val_accuracy: 0.7255 - val_loss: 0.7598\n",
      "Epoch 14/25\n",
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m277s\u001b[0m 2s/step - accuracy: 0.6510 - loss: 0.8594 - val_accuracy: 0.7074 - val_loss: 0.7338\n",
      "Epoch 15/25\n",
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m277s\u001b[0m 2s/step - accuracy: 0.6675 - loss: 0.8391 - val_accuracy: 0.7205 - val_loss: 0.7180\n",
      "Epoch 16/25\n",
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m278s\u001b[0m 2s/step - accuracy: 0.6746 - loss: 0.8285 - val_accuracy: 0.7164 - val_loss: 0.7199\n",
      "Epoch 17/25\n",
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m276s\u001b[0m 2s/step - accuracy: 0.6607 - loss: 0.8373 - val_accuracy: 0.7094 - val_loss: 0.7224\n",
      "Epoch 18/25\n",
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m277s\u001b[0m 2s/step - accuracy: 0.6734 - loss: 0.8242 - val_accuracy: 0.6993 - val_loss: 0.7307\n",
      "Epoch 19/25\n",
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m279s\u001b[0m 2s/step - accuracy: 0.6695 - loss: 0.8140 - val_accuracy: 0.7225 - val_loss: 0.7052\n",
      "Epoch 20/25\n",
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m277s\u001b[0m 2s/step - accuracy: 0.6693 - loss: 0.8159 - val_accuracy: 0.7074 - val_loss: 0.7131\n",
      "Epoch 21/25\n",
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m276s\u001b[0m 2s/step - accuracy: 0.6770 - loss: 0.8131 - val_accuracy: 0.7205 - val_loss: 0.7166\n",
      "Epoch 22/25\n",
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m278s\u001b[0m 2s/step - accuracy: 0.6728 - loss: 0.7983 - val_accuracy: 0.7195 - val_loss: 0.7011\n",
      "Epoch 23/25\n",
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m278s\u001b[0m 2s/step - accuracy: 0.6770 - loss: 0.8054 - val_accuracy: 0.7316 - val_loss: 0.6926\n",
      "Epoch 24/25\n",
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m276s\u001b[0m 2s/step - accuracy: 0.6892 - loss: 0.7907 - val_accuracy: 0.7225 - val_loss: 0.7111\n",
      "Epoch 25/25\n",
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m278s\u001b[0m 2s/step - accuracy: 0.6828 - loss: 0.7924 - val_accuracy: 0.7386 - val_loss: 0.6852\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x22f016436b0>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compile the model\n",
    "model_resnet50.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# TRAIN THE MODEL\n",
    "model_resnet50.fit(training_set, validation_data=test_set, epochs=25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MobileNetV2 Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.applications import MobileNetV2\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, GlobalAveragePooling2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 9704 images belonging to 5 classes.\n",
      "Found 991 images belonging to 5 classes.\n"
     ]
    }
   ],
   "source": [
    "# DATA PREPROCESSING\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    rotation_range=10,\n",
    "    shear_range=0.2,\n",
    "    width_shift_range=0.1,\n",
    "    height_shift_range=0.1,\n",
    "    zoom_range=0.1,\n",
    "    brightness_range=(0.8, 1.2),\n",
    "    fill_mode='nearest'\n",
    ")\n",
    "\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "training_set = train_datagen.flow_from_directory(\n",
    "    'Dataset-Wall-crack-detection',\n",
    "    target_size=(128, 128),  # MobileNetV2 input size\n",
    "    batch_size=64,\n",
    "    class_mode='categorical'\n",
    ")\n",
    "\n",
    "test_set = test_datagen.flow_from_directory(\n",
    "    'test_set',\n",
    "    target_size=(128, 128),\n",
    "    batch_size=64,\n",
    "    class_mode='categorical'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOAD PRETRAINED MobileNetV2 MODEL (Without Fully Connected Layers)\n",
    "base_model = MobileNetV2(weights='imagenet', include_top=False, input_shape=(128, 128, 3))\n",
    "\n",
    "# Freeze the convolutional layers\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = False  # Prevent updating weights\n",
    "\n",
    "# BUILD CUSTOM MODEL\n",
    "mobilenet_model = Sequential([\n",
    "    base_model,\n",
    "    GlobalAveragePooling2D(),  # Reduces dimensions\n",
    "    Dense(128, activation='relu'),\n",
    "    Dropout(0.5),  # Reduce overfitting\n",
    "    Dense(5, activation='softmax')  # 5 crack types\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m171s\u001b[0m 1s/step - accuracy: 0.6037 - loss: 1.0274 - val_accuracy: 0.7477 - val_loss: 0.6030\n",
      "Epoch 2/25\n",
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m216s\u001b[0m 1s/step - accuracy: 0.7606 - loss: 0.6038 - val_accuracy: 0.7941 - val_loss: 0.4935\n",
      "Epoch 3/25\n",
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m221s\u001b[0m 1s/step - accuracy: 0.7725 - loss: 0.5632 - val_accuracy: 0.8204 - val_loss: 0.4685\n",
      "Epoch 4/25\n",
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m160s\u001b[0m 1s/step - accuracy: 0.7824 - loss: 0.5566 - val_accuracy: 0.8315 - val_loss: 0.4437\n",
      "Epoch 5/25\n",
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m160s\u001b[0m 1s/step - accuracy: 0.7937 - loss: 0.5239 - val_accuracy: 0.8476 - val_loss: 0.4261\n",
      "Epoch 6/25\n",
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m161s\u001b[0m 1s/step - accuracy: 0.7882 - loss: 0.5293 - val_accuracy: 0.8244 - val_loss: 0.4462\n",
      "Epoch 7/25\n",
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m160s\u001b[0m 1s/step - accuracy: 0.7960 - loss: 0.5170 - val_accuracy: 0.8305 - val_loss: 0.4247\n",
      "Epoch 8/25\n",
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m159s\u001b[0m 1s/step - accuracy: 0.7956 - loss: 0.5065 - val_accuracy: 0.8385 - val_loss: 0.4103\n",
      "Epoch 9/25\n",
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m163s\u001b[0m 1s/step - accuracy: 0.8005 - loss: 0.4985 - val_accuracy: 0.8375 - val_loss: 0.4159\n",
      "Epoch 10/25\n",
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m162s\u001b[0m 1s/step - accuracy: 0.7999 - loss: 0.4987 - val_accuracy: 0.8375 - val_loss: 0.4159\n",
      "Epoch 11/25\n",
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m162s\u001b[0m 1s/step - accuracy: 0.7968 - loss: 0.5085 - val_accuracy: 0.8416 - val_loss: 0.4246\n",
      "Epoch 12/25\n",
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m161s\u001b[0m 1s/step - accuracy: 0.8119 - loss: 0.4889 - val_accuracy: 0.8527 - val_loss: 0.4088\n",
      "Epoch 13/25\n",
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m161s\u001b[0m 1s/step - accuracy: 0.8234 - loss: 0.4598 - val_accuracy: 0.8496 - val_loss: 0.4077\n",
      "Epoch 14/25\n",
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m161s\u001b[0m 1s/step - accuracy: 0.8035 - loss: 0.4913 - val_accuracy: 0.8224 - val_loss: 0.4550\n",
      "Epoch 15/25\n",
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m160s\u001b[0m 1s/step - accuracy: 0.8112 - loss: 0.4833 - val_accuracy: 0.8466 - val_loss: 0.4120\n",
      "Epoch 16/25\n",
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m160s\u001b[0m 1s/step - accuracy: 0.8169 - loss: 0.4766 - val_accuracy: 0.8476 - val_loss: 0.4206\n",
      "Epoch 17/25\n",
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m161s\u001b[0m 1s/step - accuracy: 0.8122 - loss: 0.4733 - val_accuracy: 0.8476 - val_loss: 0.3925\n",
      "Epoch 18/25\n",
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m160s\u001b[0m 1s/step - accuracy: 0.8180 - loss: 0.4582 - val_accuracy: 0.8365 - val_loss: 0.4433\n",
      "Epoch 19/25\n",
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m161s\u001b[0m 1s/step - accuracy: 0.8197 - loss: 0.4597 - val_accuracy: 0.8527 - val_loss: 0.3952\n",
      "Epoch 20/25\n",
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m161s\u001b[0m 1s/step - accuracy: 0.8207 - loss: 0.4522 - val_accuracy: 0.8507 - val_loss: 0.4103\n",
      "Epoch 21/25\n",
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m160s\u001b[0m 1s/step - accuracy: 0.8173 - loss: 0.4547 - val_accuracy: 0.8385 - val_loss: 0.4167\n",
      "Epoch 22/25\n",
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m161s\u001b[0m 1s/step - accuracy: 0.8118 - loss: 0.4567 - val_accuracy: 0.8436 - val_loss: 0.4106\n",
      "Epoch 23/25\n",
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m161s\u001b[0m 1s/step - accuracy: 0.8232 - loss: 0.4535 - val_accuracy: 0.8466 - val_loss: 0.4202\n",
      "Epoch 24/25\n",
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m160s\u001b[0m 1s/step - accuracy: 0.8286 - loss: 0.4440 - val_accuracy: 0.8345 - val_loss: 0.4308\n",
      "Epoch 25/25\n",
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m161s\u001b[0m 1s/step - accuracy: 0.8203 - loss: 0.4614 - val_accuracy: 0.8436 - val_loss: 0.4060\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    }
   ],
   "source": [
    "# Compile the model\n",
    "mobilenet_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# TRAIN THE MODEL\n",
    "mobilenet_model.fit(training_set, validation_data=test_set, epochs=25)\n",
    "\n",
    "# SAVE THE MODEL\n",
    "mobilenet_model.save(\"mobilenet_crack_detection.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# After evaluating multiple deep learning architectures, including MobileNet, ResNet50, CNN, and VGG16, we found that MobileNet achieved the highest accuracy. Therefore, we have chosen MobileNet as the backbone model for detecting wall cracks, classifying crack types (horizontal, vertical, diagonal, web-shaped), and providing suitable retrofitting techniques based on the identified crack type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3s/step\n",
      "Prediction probabilities: [[8.3931220e-01 1.0083478e-01 5.9287813e-02 4.2981910e-07 5.6485197e-04]]\n",
      "Predicted Crack Type: Spiderweb Cracks\n",
      "Likely Cause of Crack: Poor curing, material defects\n",
      "Retrofitting Techniques:\n",
      "- Re-plaster the affected area with quality materials.\n",
      "- Use non-shrink grout or fillers to seal gaps.\n",
      "- Conduct proper curing of concrete to prevent recurrence.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from keras.preprocessing import image\n",
    "\n",
    "# Load and preprocess the test image\n",
    "test_image = image.load_img('single-prediction/123456.jpg', target_size=(128, 128))  # Match CNN input size\n",
    "test_image = image.img_to_array(test_image)\n",
    "test_image = test_image / 255.0  # Normalize\n",
    "test_image = np.expand_dims(test_image, axis=0)\n",
    "\n",
    "# Predict the crack type\n",
    "result = mobilenet_model.predict(test_image)\n",
    "print(\"Prediction probabilities:\", result)\n",
    "\n",
    "# Mapping of classes to crack types, causes, and retrofitting techniques\n",
    "class_mapping = {\n",
    "    0: {\n",
    "        'type': 'Spiderweb Cracks',\n",
    "        'cause': 'Poor curing, material defects',\n",
    "        'retrofit': [\n",
    "            'Re-plaster the affected area with quality materials.',\n",
    "            'Use non-shrink grout or fillers to seal gaps.',\n",
    "            'Conduct proper curing of concrete to prevent recurrence.'\n",
    "        ]\n",
    "    },\n",
    "    1: {\n",
    "        'type': 'Diagonal Cracks',\n",
    "        'cause': 'Seismic forces, differential settlement',\n",
    "        'retrofit': [\n",
    "            'Apply carbon fiber strips for reinforcement.',\n",
    "            'Use steel mesh with mortar for re-strengthening.',\n",
    "            'Re-level the structure if caused by settlement.'\n",
    "        ]\n",
    "    },\n",
    "    2: {\n",
    "        'type': 'Horizontal Cracks',\n",
    "        'cause': 'Settlement, soil deformation, thermal stress',\n",
    "        'retrofit': [\n",
    "            'Install tie rods or straps to reinforce walls.',\n",
    "            'Apply epoxy injection to bond cracks.',\n",
    "            'Add support beams or bracing to reduce stress.'\n",
    "        ]\n",
    "    },\n",
    "    3: {\n",
    "        'type': 'Non-crack',\n",
    "        'cause': 'Crack not present',\n",
    "        'retrofit': ['Retrofitting not required.']\n",
    "    },\n",
    "    4: {\n",
    "        'type': 'Vertical Cracks',\n",
    "        'cause': 'Shrinkage, uneven foundation, overload',\n",
    "        'retrofit': [\n",
    "            'Use polyurethane or epoxy injection for sealing.',\n",
    "            'Strengthen foundations with underpinning.',\n",
    "            'Improve soil compaction around the foundation.'\n",
    "        ]\n",
    "    },\n",
    "    5: {\n",
    "        'type': 'Hairline Cracks',\n",
    "        'cause': 'Shrinkage, minor settlement',\n",
    "        'retrofit': [\n",
    "            'Use surface sealants like acrylic or epoxy.',\n",
    "            'Monitor cracks for progression over time.',\n",
    "            'Improve environmental conditions to reduce shrinkage.'\n",
    "        ]\n",
    "    },\n",
    "    6: {\n",
    "        'type': 'Step Cracks',\n",
    "        'cause': 'Foundation movement, weak mortar joints',\n",
    "        'retrofit': [\n",
    "            'Replace damaged mortar with fresh mortar.',\n",
    "            'Strengthen masonry with steel reinforcement.',\n",
    "            'Stabilize the foundation to prevent further movement.'\n",
    "        ]\n",
    "    },\n",
    "    7: {\n",
    "        'type': 'Wide/Deep Cracks',\n",
    "        'cause': 'Structural instability, heavy loads',\n",
    "        'retrofit': [\n",
    "            'Install steel or concrete anchors to stabilize the structure.',\n",
    "            'Use epoxy injection for sealing.',\n",
    "            'Reinforce with additional steel rods or plates.'\n",
    "        ]\n",
    "    }\n",
    "}\n",
    "\n",
    "# Get the predicted class index\n",
    "predicted_class = np.argmax(result)\n",
    "\n",
    "# Handle predictions and display retrofitting techniques\n",
    "if predicted_class in class_mapping:\n",
    "    prediction = class_mapping[predicted_class]\n",
    "    print(f\"Predicted Crack Type: {prediction['type']}\")\n",
    "    print(f\"Likely Cause of Crack: {prediction['cause']}\")\n",
    "    print(\"Retrofitting Techniques:\")\n",
    "    for technique in prediction['retrofit']:\n",
    "        print(f\"- {technique}\")\n",
    "else:\n",
    "    print(\"Prediction confidence too low or unknown class!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "filename = 'training_set.sav'\n",
    "pickle.dump(cnn, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lenovo\\anaconda3\\Lib\\site-packages\\keras\\src\\saving\\saving_lib.py:713: UserWarning: Skipping variable loading for optimizer 'adam', because it has 22 variables whereas the saved optimizer has 2 variables. \n",
      "  saveable.load_own_variables(weights_store.get(inner_path))\n"
     ]
    }
   ],
   "source": [
    "# loading the saved model\n",
    "loaded_model = pickle.load(open('training_set.sav', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 134ms/step\n",
      "Prediction probabilities: [[9.9223000e-01 4.0314151e-03 3.7379717e-03 2.5335298e-08 5.7651545e-07]]\n",
      "Predicted Crack Type: Spiderweb Cracks\n",
      "Likely Cause of Crack: Poor curing, material defects\n",
      "Retrofitting Techniques:\n",
      "- Re-plaster the affected area with quality materials.\n",
      "- Use non-shrink grout or fillers to seal gaps.\n",
      "- Conduct proper curing of concrete to prevent recurrence.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from keras.preprocessing import image\n",
    "\n",
    "# Load and preprocess the test image\n",
    "test_image = image.load_img('single-prediction/123456.jpg', target_size=(128, 128))  # Match CNN input size\n",
    "test_image = image.img_to_array(test_image)\n",
    "test_image = test_image / 255.0  # Normalize\n",
    "test_image = np.expand_dims(test_image, axis=0)\n",
    "\n",
    "# Predict the crack type\n",
    "result = loaded_model.predict(test_image)\n",
    "print(\"Prediction probabilities:\", result)\n",
    "\n",
    "# Mapping of classes to crack types, causes, and retrofitting techniques\n",
    "class_mapping = {\n",
    "    0: {\n",
    "        'type': 'Spiderweb Cracks',\n",
    "        'cause': 'Poor curing, material defects',\n",
    "        'retrofit': [\n",
    "            'Re-plaster the affected area with quality materials.',\n",
    "            'Use non-shrink grout or fillers to seal gaps.',\n",
    "            'Conduct proper curing of concrete to prevent recurrence.'\n",
    "        ]\n",
    "    },\n",
    "    1: {\n",
    "        'type': 'Diagonal Cracks',\n",
    "        'cause': 'Seismic forces, differential settlement',\n",
    "        'retrofit': [\n",
    "            'Apply carbon fiber strips for reinforcement.',\n",
    "            'Use steel mesh with mortar for re-strengthening.',\n",
    "            'Re-level the structure if caused by settlement.'\n",
    "        ]\n",
    "    },\n",
    "    2: {\n",
    "        'type': 'Horizontal Cracks',\n",
    "        'cause': 'Settlement, soil deformation, thermal stress',\n",
    "        'retrofit': [\n",
    "            'Install tie rods or straps to reinforce walls.',\n",
    "            'Apply epoxy injection to bond cracks.',\n",
    "            'Add support beams or bracing to reduce stress.'\n",
    "        ]\n",
    "    },\n",
    "    3: {\n",
    "        'type': 'Non-crack',\n",
    "        'cause': 'Crack not present',\n",
    "        'retrofit': ['Retrofitting not required.']\n",
    "    },\n",
    "    4: {\n",
    "        'type': 'Vertical Cracks',\n",
    "        'cause': 'Shrinkage, uneven foundation, overload',\n",
    "        'retrofit': [\n",
    "            'Use polyurethane or epoxy injection for sealing.',\n",
    "            'Strengthen foundations with underpinning.',\n",
    "            'Improve soil compaction around the foundation.'\n",
    "        ]\n",
    "    },\n",
    "    5: {\n",
    "        'type': 'Hairline Cracks',\n",
    "        'cause': 'Shrinkage, minor settlement',\n",
    "        'retrofit': [\n",
    "            'Use surface sealants like acrylic or epoxy.',\n",
    "            'Monitor cracks for progression over time.',\n",
    "            'Improve environmental conditions to reduce shrinkage.'\n",
    "        ]\n",
    "    },\n",
    "    6: {\n",
    "        'type': 'Step Cracks',\n",
    "        'cause': 'Foundation movement, weak mortar joints',\n",
    "        'retrofit': [\n",
    "            'Replace damaged mortar with fresh mortar.',\n",
    "            'Strengthen masonry with steel reinforcement.',\n",
    "            'Stabilize the foundation to prevent further movement.'\n",
    "        ]\n",
    "    },\n",
    "    7: {\n",
    "        'type': 'Wide/Deep Cracks',\n",
    "        'cause': 'Structural instability, heavy loads',\n",
    "        'retrofit': [\n",
    "            'Install steel or concrete anchors to stabilize the structure.',\n",
    "            'Use epoxy injection for sealing.',\n",
    "            'Reinforce with additional steel rods or plates.'\n",
    "        ]\n",
    "    }\n",
    "}\n",
    "\n",
    "# Get the predicted class index\n",
    "predicted_class = np.argmax(result)\n",
    "\n",
    "# Handle predictions and display retrofitting techniques\n",
    "if predicted_class in class_mapping:\n",
    "    prediction = class_mapping[predicted_class]\n",
    "    print(f\"Predicted Crack Type: {prediction['type']}\")\n",
    "    print(f\"Likely Cause of Crack: {prediction['cause']}\")\n",
    "    print(\"Retrofitting Techniques:\")\n",
    "    for technique in prediction['retrofit']:\n",
    "        print(f\"- {technique}\")\n",
    "else:\n",
    "    print(\"Prediction confidence too low or unknown class!\")\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
